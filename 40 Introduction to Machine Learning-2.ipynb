{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2545ed5e-b830-42ef-973f-25f120eecac1",
   "metadata": {},
   "source": [
    "## Introduction to Machine Learning-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dfeeae-52a4-4011-adb9-06f951c241b6",
   "metadata": {},
   "source": [
    "## Q1: Define overfitting and underfitting in machine learning. What are the consequences of each, and how can they be mitigated?\n",
    "\n",
    "Overfitting occurs when a machine learning model fits the training data too closely, capturing the noise and the specifics of the training data that do not generalize well to the new data. It often results in high accuracy on the training data but poor performance on new data. Underfitting, on the other hand, occurs when the model is too simple to capture the underlying patterns in the data, resulting in poor accuracy on both training and new data. To mitigate overfitting, one can use techniques such as regularization, early stopping, or dropout, while increasing model complexity, adding more features, or reducing regularization can help mitigate underfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2347b890-2697-4d73-b983-5427bd0654ee",
   "metadata": {},
   "source": [
    "## Q2: How can we reduce overfitting? Explain in brief.\n",
    "\n",
    "To reduce overfitting in machine learning, one can use techniques such as regularization, early stopping, or dropout. Regularization adds a penalty term to the loss function to prevent the model from overemphasizing certain features or overfitting the training data. Early stopping stops the training process before the model overfits the training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3705f4a5-15a0-4c28-873a-31711baa3ba1",
   "metadata": {},
   "source": [
    "## Q3: Explain underfitting. List scenarios where underfitting can occur in ML.\n",
    "\n",
    "Underfitting occurs when the model is too simple to capture the underlying patterns in the data, resulting in poor accuracy on both training and new data. It can occur in scenarios such as using a linear model to fit a nonlinear problem, using too few features, or applying too much regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae6c002-1620-4279-ae29-3fcfdd0c230b",
   "metadata": {},
   "source": [
    "## Q4: Explain the bias-variance tradeoff in machine learning. What is the relationship between bias and variance, and how do they affect model performance?\n",
    "\n",
    "The bias-variance tradeoff is a fundamental concept in machine learning that refers to the tradeoff between the model's ability to fit the training data well (low bias) and its ability to generalize to new data (low variance). High bias models are typically too simple and cannot capture the underlying patterns in the data, while high variance models are often too complex and overfit the training data. Finding the right balance between bias and variance is critical for achieving optimal model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013353d6-76a8-424e-b831-95c4e45a50fa",
   "metadata": {},
   "source": [
    "## Q5: Discuss some common methods for detecting overfitting and underfitting in machine learning models. How can you determine whether your model is overfitting or underfitting?\n",
    "\n",
    "Common methods for detecting overfitting and underfitting in machine learning models include analyzing the training and validation error, using learning curves, or using techniques such as cross-validation. If the training error is low while the validation error is high, the model is likely overfitting. If both errors are high, the model is likely underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aec5c643-5c3d-4330-b488-a9c56e4379b7",
   "metadata": {},
   "source": [
    "## Q6: Compare and contrast bias and variance in machine learning. What are some examples of high bias and high variance models, and how do they differ in terms of their performance?\n",
    "\n",
    "Bias refers to the error introduced by approximating a real-life problem with a simpler model, while variance refers to the error introduced by the model's sensitivity to small fluctuations in the training data. High bias models are typically too simple and cannot capture the underlying patterns in the data, while high variance models are often too complex and overfit the training data. For example, a linear regression model may have high bias, while a decision tree model may have high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6824d19-2af8-41b8-8f19-a8a9ee2d80ee",
   "metadata": {},
   "source": [
    "## Q7: What is regularization in machine learning, and how can it be used to prevent overfitting? Describe some common regularization techniques and how they work.\n",
    "\n",
    "Regularization is a technique used in machine learning to prevent overfitting by adding a penalty term to the loss function. Common regularization techniques include L1 and L2 regularization, dropout, and early stopping. L1 regularization adds a penalty proportional to the absolute value of the weights, promoting sparsity, while L2 regularization adds a penalty proportional to the square of the weights, promoting small weights. Dropout randomly drops out units or neurons during training, while early stopping stops the training process before the model overfits the training data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
