{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07eb61e2-4c12-475a-8ed8-d7f43d0a1577",
   "metadata": {},
   "source": [
    "# KNN-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e38cb99c-23a1-4899-aed7-641d427da7fa",
   "metadata": {},
   "source": [
    "### Q1. What is the KNN algorithm?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ad7513-c053-4f75-be81-13ad06d6c566",
   "metadata": {},
   "source": [
    "The K-Nearest Neighbors (KNN) algorithm is a supervised machine learning algorithm used for both classification and regression tasks. It is a non-parametric algorithm that makes predictions based on the similarity of input data points to the labeled data in the training set. In KNN, the \"K\" represents the number of nearest neighbors to consider when making a prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271f47f0-4432-40ee-b207-b383a4b3a82d",
   "metadata": {},
   "source": [
    "### Q2. How do you choose the value of K in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6332cc07-d5dc-4c30-a9ff-a763ced5c39e",
   "metadata": {},
   "source": [
    "The value of K in KNN is chosen through a process called hyperparameter tuning. It depends on the dataset and problem at hand. A smaller value of K, such as 1, can lead to more flexible decision boundaries but may be sensitive to noise. A larger value of K can provide smoother decision boundaries but may lead to the loss of local patterns. The choice of K is typically determined by using techniques like cross-validation to find the value that produces the best performance on the validation set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2ab9fd-6b14-416c-8646-3049695718d8",
   "metadata": {},
   "source": [
    "### Q3. What is the difference between KNN classifier and KNN regressor?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6201aae6-4a85-405e-b543-a6136f51b1ed",
   "metadata": {},
   "source": [
    "The main difference between the KNN classifier and KNN regressor lies in the type of problem they are used to solve.\n",
    "\n",
    "- KNN classifier: It is used for classification problems, where the goal is to assign a categorical label to an input data point. In KNN classification, the class label of a query point is determined by majority voting among its K nearest neighbors. The predicted label is the one that occurs most frequently among the K neighbors.\n",
    "\n",
    "- KNN regressor: It is used for regression problems, where the goal is to predict a continuous numerical value. In KNN regression, the predicted value for a query point is computed as the average or weighted average of the target values of its K nearest neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e644207-0ab6-4546-91a7-65a62d3a7125",
   "metadata": {},
   "source": [
    "### Q4. How do you measure the performance of KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979e0041-d39b-4d3a-8cc8-cf9442a5c864",
   "metadata": {},
   "source": [
    "The performance of KNN can be measured using various evaluation metrics depending on the task at hand (classification or regression). Common metrics for classification include accuracy, precision, recall, F1 score, and area under the receiver operating characteristic curve (AUC-ROC). For regression tasks, popular metrics include mean squared error (MSE), mean absolute error (MAE), and R-squared (coefficient of determination)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba53543-3ad3-4388-bee1-ab87a45f5f15",
   "metadata": {},
   "source": [
    "### Q5. What is the curse of dimensionality in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d796ba3f-d153-42d6-b61f-65d05de3de99",
   "metadata": {},
   "source": [
    "The curse of dimensionality in KNN refers to the phenomenon where the performance of the algorithm deteriorates as the number of features or dimensions in the data increases. As the number of dimensions increases, the volume of the feature space grows exponentially, resulting in sparse data. In such cases, it becomes difficult to find meaningful nearest neighbors because the concept of proximity loses its effectiveness. The curse of dimensionality can lead to increased computation time and decreased predictive accuracy. Dimensionality reduction techniques and feature selection methods can help mitigate this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318ac3b0-307d-48cc-85cb-8dd321a9ed7e",
   "metadata": {},
   "source": [
    "### Q6. How do you handle missing values in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09632449-07e4-49e9-9bd6-4be65e5988de",
   "metadata": {},
   "source": [
    "Handling missing values in KNN can be done by imputing them with estimated values. There are several strategies to handle missing values:\n",
    "\n",
    "- Simplest approach: Remove data points with missing values. However, this can lead to a loss of valuable information.\n",
    "- Mean or median imputation: Replace missing values with the mean or median value of the corresponding feature across the available data.\n",
    "- KNN imputation: Use the KNN algorithm to estimate missing values by considering the feature values of the K nearest neighbors of the data point with the missing value.\n",
    "- Advanced techniques: Utilize more sophisticated methods like matrix factorization, multiple imputation, or probabilistic models for handling missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5446df71-6f50-4146-8345-9fc754cf0404",
   "metadata": {},
   "source": [
    "### Q7. Compare and contrast the performance of the KNN classifier and regressor. Which one is better for which type of problem?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b79c57ce-459f-4ad6-b91c-2c12cc47258f",
   "metadata": {},
   "source": [
    "The performance of the KNN classifier and KNN regressor depends on the problem at hand:\n",
    "\n",
    "- KNN classifier: It is better suited for problems where the target variable is categorical and the goal is to assign class labels. It can handle both binary and multi-class classification tasks.\n",
    "\n",
    "- KNN regressor: It is better suited for problems where the target variable is continuous and the goal is to predict numerical values. It can be used for tasks like predicting housing prices, stock prices, or any other continuous variable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e023405c-8472-42f0-a0f4-0e739f3ad6bd",
   "metadata": {},
   "source": [
    "### Q8. What are the strengths and weaknesses of the KNN algorithm for classification and regression tasks, and how can these be addressed?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643c2e56-c10f-4e87-a092-23d958051d15",
   "metadata": {},
   "source": [
    "Strengths and weaknesses of the KNN algorithm:\n",
    "\n",
    "- Strengths:\n",
    "\n",
    "1. Simple and intuitive algorithm.\n",
    "2. Can be used for both classification and regression tasks.\n",
    "3. Non-parametric nature allows it to capture complex relationships between variables.\n",
    "4. Robust to noisy training data.\n",
    "5. Can handle multi-class classification.\n",
    "\n",
    "- Weaknesses:\n",
    "\n",
    "1. Computationally expensive during prediction, especially for large datasets.\n",
    "2. Sensitivity to the choice of K.\n",
    "3. Requires a meaningful distance metric for accurate results.\n",
    "4. Curse of dimensionality can adversely affect performance in high-dimensional spaces.\n",
    "5. Lack of interpretability in the decision-making process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9ba2aa-cdc7-4f1e-b7bf-2ea30aa287d6",
   "metadata": {},
   "source": [
    "### Q9. What is the difference between Euclidean distance and Manhattan distance in KNN?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51ddd4c-ba02-4688-a553-5c3046d60205",
   "metadata": {},
   "source": [
    "- Euclidean distance: It is the straight-line or \"as-the-crow-flies\" distance between two points in a multidimensional space. In Euclidean distance, the distance between two points is calculated as the square root of the sum of the squared differences between their corresponding coordinates.\n",
    "\n",
    "- Manhattan distance: It is also known as city block distance or L1 distance. In Manhattan distance, the distance between two points is calculated as the sum of the absolute differences between their corresponding coordinates. It represents the distance traveled along the grid-like streets of a city when moving from one point to another."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52532375-0e8e-414f-9415-201ba3646f88",
   "metadata": {},
   "source": [
    "### Q10. What is the role of feature scaling in KNN?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078b5626-34e5-402c-a9f2-6b947823c21d",
   "metadata": {},
   "source": [
    "By scaling the features, all features are brought to a similar scale, preventing any single feature from disproportionately influencing the distance metric. Common techniques for feature scaling include standardization (mean centering and scaling by the standard deviation) and normalization (scaling to a specific range, such as [0, 1]). The choice of scaling method depends on the distribution of the data and the requirements of the problem. Feature scaling is generally recommended before applying KNN to ensure fair comparisons between features and accurate distance calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "023d1871-485a-4fe3-b50a-6dbddee881ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
