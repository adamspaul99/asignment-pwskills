{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24dd9b84-2c72-4d07-8367-3a06ee312139",
   "metadata": {},
   "source": [
    "# Regression-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0c15a8-a94b-40e2-be56-335500de4179",
   "metadata": {},
   "source": [
    "### Q1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "Lasso Regression is a linear regression technique used for feature selection and regularization. It differs from other regression techniques in that it adds a penalty term to the ordinary least squares objective function, where the penalty is the L1 norm of the coefficients. This penalty encourages the model to have sparse coefficients, i.e., it forces some of the coefficients to be exactly zero, effectively removing some of the features from the model.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c230cfcd-3ad1-4b5b-a958-e0a140d03834",
   "metadata": {},
   "source": [
    "### Q2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "The main advantage of using Lasso Regression in feature selection is that it can automatically select relevant features and discard irrelevant ones, leading to a simpler and more interpretable model. This is especially useful when dealing with high-dimensional data where the number of features is much larger than the number of observations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "593a38ec-a17a-467a-8e33-b8b29ffaeb36",
   "metadata": {},
   "source": [
    "### Q3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "The coefficients of a Lasso Regression model can be interpreted in the same way as the coefficients of a linear regression model. However, due to the L1 penalty, some of the coefficients may be exactly zero, indicating that the corresponding features have been removed from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306639dd-048a-4564-b0c0-8a08dd0264e8",
   "metadata": {},
   "source": [
    "### Q4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
    "The tuning parameter in Lasso Regression is the regularization parameter (lambda), which controls the strength of the L1 penalty. A larger value of lambda leads to a sparser model with more coefficients set to zero, while a smaller value of lambda allows more coefficients to be non-zero. The optimal value of lambda can be selected using cross-validation, where the value of lambda that gives the best performance on a validation set is chosen.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f8a696-9ffc-4774-a346-60feda2f6304",
   "metadata": {},
   "source": [
    "### Q5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "Lasso Regression can be used for non-linear regression problems by transforming the input features using non-linear functions such as polynomials, radial basis functions, or splines. However, it may not perform as well as other non-linear regression techniques such as decision trees, random forests, or neural networks.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bfaf68-fc5e-45bf-92f2-69591fbe18d4",
   "metadata": {},
   "source": [
    "### Q6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "The main difference between Ridge Regression and Lasso Regression is the penalty term used in the objective function. Ridge Regression uses the L2 norm of the coefficients as the penalty, which encourages the model to have small but non-zero coefficients, while Lasso Regression uses the L1 norm of the coefficients, which encourages the model to have sparse coefficients with some exactly equal to zero.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b361d4-bcc3-40dc-8cd3-5af0f632fcf8",
   "metadata": {},
   "source": [
    "### Q7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "Lasso Regression can handle multicollinearity in the input features by selecting only one of the correlated features and setting the coefficients of the other correlated features to zero. This is because the L1 penalty encourages sparsity and feature selection, effectively removing redundant features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51634173-27a5-4cfd-a2f2-7e011c05b959",
   "metadata": {},
   "source": [
    "### Q8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?\n",
    "The optimal value of the regularization parameter (lambda) in Lasso Regression can be chosen using cross-validation, where the value of lambda that gives the best performance on a validation set is selected. Alternatively, one can use techniques such as the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC) to choose the value of lambda that balances model complexity and goodness of fit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
