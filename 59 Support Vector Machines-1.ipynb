{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b555ef3-b1b6-48f1-bf2b-863bbdd7c827",
   "metadata": {},
   "source": [
    "# Support Vector Machines-1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657674e7-0e54-490a-a172-865998d0ee0c",
   "metadata": {},
   "source": [
    "### Q1. What is the mathematical formula for a linear SVM?\n",
    "f(x) = w^T x + b\n",
    "\n",
    "where,\n",
    "- x is the input feature vector, \n",
    "- w is the weight vector, \n",
    "- b is the bias, \n",
    "- ^T denotes the transpose operation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da646cd2-7bb2-43e9-b219-23ed35818cbe",
   "metadata": {},
   "source": [
    "### Q2. What is the objective function of a linear SVM?\n",
    "\n",
    "minimize 1/2 ||w||^2\n",
    "\n",
    "subject to y_i(w^T x_i + b) >= 1 for all i\n",
    "where ||w||^2 is the L2-norm of the weight vector, y_i is the target class label of the i-th training example, and x_i is its feature vector. The constraint y_i(w^T x_i + b) >= 1 enforces that the margin between the two classes is at least 1.\n",
    "\n",
    "i use google search for this question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a48a1ae4-ae76-481b-8e2c-89353a99b609",
   "metadata": {},
   "source": [
    "### Q3. What is the kernel trick in SVM?\n",
    "The kernel trick in SVM is a technique that allows us to implicitly map the input features into a high-dimensional feature space. The idea is to replace the dot product x_i^T x_j in the objective function with a kernel function K(x_i, x_j), which measures the similarity between the two input vectors in the high-dimensional feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cd45a3-1602-40db-ab5c-58cdc63054ac",
   "metadata": {},
   "source": [
    "### Q4. What is the role of support vectors in SVM Explain with example\n",
    "The support vectors in SVM are the training examples that lie on or within the margin boundary. These examples play a critical role in defining the decision boundary of the classifier. Once the SVM model is trained, only the support vectors are needed to make predictions on new examples.\n",
    "\n",
    "For example, in a binary classification problem with two classes, the decision boundary is defined by the hyperplane that separates the two classes and is equidistant from the closest support vectors on either side."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6183861a-520b-4a69-ac82-6cb98ea4d3b5",
   "metadata": {},
   "source": [
    "### Q5. Illustrate with examples and graphs of Hyperplane, Marginal plane, Soft margin and Hard margin in SVM?\n",
    "- Hyperplane: In a binary classification problem, the hyperplane is the linear decision boundary that separates the two classes. For example, in a 2D input space, the hyperplane is a line, and in a 3D input space, it is a plane.\n",
    "\n",
    "- Marginal plane: The marginal plane is the plane that is parallel to the hyperplane and just touches the support vectors on either side. The distance between the hyperplane and the marginal plane is the margin.\n",
    "\n",
    "- Soft margin: Soft margin SVM allows some misclassifications in the training set by introducing a slack variable ξ_i >= 0 for each training example i.\n",
    "\n",
    "- Hard margin: Hard margin SVM requires that all training examples are correctly classified, i.e., there are no misclassifications. The objective function is the same as for soft margin SVM, but with C = ∞."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
