{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5de50d35-bcf4-4f08-aec9-c0f06f1e0fa7",
   "metadata": {},
   "source": [
    "# Support Vector Machines-2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95348e27-bced-45cb-a1ae-0408cd00e68c",
   "metadata": {},
   "source": [
    "### Q1. What is the relationship between polynomial functions and kernel functions in machine learning algorithms?\n",
    " Polynomial functions and kernel functions are both used in machine learning algorithms, particularly in Support Vector Machines (SVMs). In SVMs, a kernel function is used to transform the data from its original feature space to a higher-dimensional space, where it is easier to separate the data into different classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5edda591-5e1e-4023-837b-55dcd35c7082",
   "metadata": {},
   "source": [
    "### Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6242463c-4089-4ea3-be93-8d86737016eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVR(coef0=1, degree=2, kernel=&#x27;poly&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVR</label><div class=\"sk-toggleable__content\"><pre>SVR(coef0=1, degree=2, kernel=&#x27;poly&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVR(coef0=1, degree=2, kernel='poly')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "import numpy as np\n",
    "\n",
    "# generate some random data\n",
    "X = np.random.rand(100, 2)\n",
    "y = 2*X[:,0] - 3*X[:,1] + 1 + 0.1*np.random.randn(100)\n",
    "\n",
    "# create an SVM with a polynomial kernel of degree 2\n",
    "clf = svm.SVR(kernel='poly', degree=2, coef0=1)\n",
    "\n",
    "# fit the SVM to the data\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d11ac86-52c1-4e3e-97bc-9804a4f5efa8",
   "metadata": {},
   "source": [
    "### Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "Increasing the value of epsilon in Support Vector Regression (SVR) will typically increase the number of support vectors in the model. This is because increasing epsilon allows more points to fall within the margin of the SVR, and therefore more points may be considered support vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddc75ce6-5048-45be-98fc-988423dae68e",
   "metadata": {},
   "source": [
    "### Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works and provide examples of when you might want to increase or decrease its value?\n",
    "\n",
    "- Kernel function: The choice of kernel function determines how the data is mapped to a higher-dimensional space, and can therefore affect the model's ability to capture complex patterns in the data. For example, a polynomial kernel may be appropriate for data with non-linear patterns, while a linear kernel may be more appropriate for linearly separable data.\n",
    "\n",
    "- C parameter: The C parameter determines the trade-off between maximizing the margin and minimizing the classification error. A smaller value of C will result in a wider margin but may allow more misclassifications, while a larger value of C will result in a narrower margin but may have fewer misclassifications.\n",
    "\n",
    "- Epsilon parameter: The epsilon parameter determines the width of the epsilon-tube around the regression line within which no penalty is associated with errors. Increasing epsilon will allow more points to fall within the margin and may increase the number of support vectors in the model.\n",
    "\n",
    "- Gamma parameter: The gamma parameter determines the shape of the decision boundary and can affect the model's ability to capture complex patterns in the data. A smaller value of gamma will result in a smoother decision boundary, while a larger value of gamma will result in a more complex decision boundary that may overfit the data.\n",
    "\n",
    "\n",
    "For example, if the data has a non-linear pattern, a polynomial kernel with a higher degree may be appropriate. If the goal is to minimize the number of misclassifications, a larger value of C may be appropriate, while a smaller value of gamma may result in a smoother decision boundary that generalizes better to new data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4ecd92-1d14-4eff-8194-34485e9399e6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
